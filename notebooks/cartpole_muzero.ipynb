{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.mcts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuZero:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def representation_model(self):\n",
    "        pass\n",
    "\n",
    "    def dynamics_model(self):\n",
    "        pass\n",
    "    \n",
    "    def prediction_model(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, episodes = 0):\n",
    "    \n",
    "    episodic_reward = 0\n",
    "    for _ in range(episodes): # episodes\n",
    "        state = env.reset()\n",
    "        terminal = False\n",
    "        cum_reward = 0\n",
    "        \n",
    "        while not terminal: # steps\n",
    "            env.render()\n",
    "            \n",
    "            #choose action\n",
    "            action = np.random.choice([0,1])\n",
    "            \n",
    "            \n",
    "            #print(f\"{action} chosen.\")\n",
    "            state, reward, terminal, done, info = env.step(action)\n",
    "            cum_reward += reward\n",
    "            \n",
    "            # update model\n",
    "\n",
    "        print(\"Cumulative Reward: \" , cum_reward)\n",
    "        episodic_reward += cum_reward\n",
    "    print(f\"average cumulative reward: {episodic_reward/episodes}\")\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\",  new_step_api = True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Reward:  29.0\n",
      "Cumulative Reward:  9.0\n",
      "Cumulative Reward:  12.0\n",
      "Cumulative Reward:  17.0\n",
      "Cumulative Reward:  18.0\n",
      "Cumulative Reward:  22.0\n",
      "Cumulative Reward:  33.0\n",
      "Cumulative Reward:  17.0\n",
      "Cumulative Reward:  15.0\n",
      "Cumulative Reward:  13.0\n",
      "average cumulative reward: 18.5\n"
     ]
    }
   ],
   "source": [
    "env = run(env,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01923349, -0.04764029,  0.02074927, -0.00664691], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('ReconChess-heQhzAHB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "844e67d598a86a7fdd81dfb15f695fdd722aa4260201ad17d5f96296f29c105e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
